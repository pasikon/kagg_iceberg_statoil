{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/bluevalhalla/fully-convolutional-network-lb-0-193\n",
    "# Random initialization\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# Uncomment this to hide TF warnings about allocation\n",
    "#import os\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# An image clearing dependencies\n",
    "from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,\n",
    "                                 denoise_wavelet, estimate_sigma, denoise_tv_bregman, denoise_nl_means)\n",
    "from skimage.filters import gaussian\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "# Data reading and visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Training part\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, GlobalAveragePooling2D, Lambda\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(98643)\n",
    "tf.set_random_seed(683)\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate data to an image format\n",
    "def color_composite(data):\n",
    "    rgb_arrays = []\n",
    "    for i, row in data.iterrows():\n",
    "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
    "        band_3 = band_1 / band_2\n",
    "\n",
    "        r = (band_1 + abs(band_1.min())) / np.max((band_1 + abs(band_1.min())))\n",
    "        g = (band_2 + abs(band_2.min())) / np.max((band_2 + abs(band_2.min())))\n",
    "        b = (band_3 + abs(band_3.min())) / np.max((band_3 + abs(band_3.min())))\n",
    "\n",
    "        rgb = np.dstack((r, g, b))\n",
    "        rgb_arrays.append(rgb)\n",
    "    return np.array(rgb_arrays)\n",
    "\n",
    "\n",
    "def denoise(X, weight, multichannel):\n",
    "    return np.asarray([denoise_tv_chambolle(item, weight=weight, multichannel=multichannel) for item in X])\n",
    "\n",
    "\n",
    "def smooth(X, sigma):\n",
    "    return np.asarray([gaussian(item, sigma=sigma) for item in X])\n",
    "\n",
    "\n",
    "def grayscale(X):\n",
    "    return np.asarray([rgb2gray(item) for item in X])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json(\"../input/train.json\")\n",
    "train.inc_angle = train.inc_angle.replace('na', 0)\n",
    "train.inc_angle = train.inc_angle.astype(float).fillna(0.0)\n",
    "train_all = True\n",
    "\n",
    "# These are train flags that required to train model more efficiently and \n",
    "# select proper model parameters\n",
    "train_b = True or train_all\n",
    "train_img = True or train_all\n",
    "train_total = True or train_all\n",
    "predict_submission = True and train_all\n",
    "\n",
    "clean_all = False\n",
    "clean_b = False or clean_all\n",
    "clean_img = False or clean_all\n",
    "\n",
    "load_all = False\n",
    "load_b = False or load_all\n",
    "load_img = False or load_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(frame, labeled, smooth_rgb=0.2, smooth_gray=0.5,\n",
    "                   weight_rgb=0.05, weight_gray=0.05):\n",
    "    band_1, band_2, images = frame['band_1'].values, frame['band_2'].values, color_composite(frame)\n",
    "    to_arr = lambda x: np.asarray([np.asarray(item) for item in x])\n",
    "    band_1 = to_arr(band_1)\n",
    "    band_2 = to_arr(band_2)\n",
    "    band_3 = (band_1 + band_2) / 2\n",
    "    gray_reshape = lambda x: np.asarray([item.reshape(75, 75) for item in x])\n",
    "    # Make a picture format from flat vector\n",
    "    band_1 = gray_reshape(band_1)\n",
    "    band_2 = gray_reshape(band_2)\n",
    "    band_3 = gray_reshape(band_3)\n",
    "    print('Denoising and reshaping')\n",
    "    if train_b and clean_b:\n",
    "        # Smooth and denoise data\n",
    "        band_1 = smooth(denoise(band_1, weight_gray, False), smooth_gray)\n",
    "        print('Gray 1 done')\n",
    "        band_2 = smooth(denoise(band_2, weight_gray, False), smooth_gray)\n",
    "        print('Gray 2 done')\n",
    "        band_3 = smooth(denoise(band_3, weight_gray, False), smooth_gray)\n",
    "        print('Gray 3 done')\n",
    "    if train_img and clean_img:\n",
    "        images = smooth(denoise(images, weight_rgb, True), smooth_rgb)\n",
    "    print('RGB done')\n",
    "    tf_reshape = lambda x: np.asarray([item.reshape(75, 75, 1) for item in x])\n",
    "    band_1 = tf_reshape(band_1)\n",
    "    band_2 = tf_reshape(band_2)\n",
    "    band_3 = tf_reshape(band_3)\n",
    "    #images = tf_reshape(images)\n",
    "    band = np.concatenate([band_1, band_2, band_3], axis=3)\n",
    "    if labeled:\n",
    "        y = np.array(frame[\"is_iceberg\"])\n",
    "    else:\n",
    "        y = None\n",
    "    return y, band, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_b, X_images = create_dataset(train, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(200, figsize=(15, 15))\n",
    "random_indicies = np.random.choice(range(len(X_images)), 9, False)\n",
    "subset = X_images[random_indicies]\n",
    "for i in range(9):\n",
    "    ax = fig.add_subplot(3, 3, i + 1)\n",
    "    ax.imshow(subset[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(202, figsize=(15, 15))\n",
    "band_1_x = train['band_1'].values\n",
    "subset = np.asarray(band_1_x)[random_indicies]\n",
    "subset = np.asarray([np.asarray(item).reshape(75, 75) for item in subset])\n",
    "for i in range(9):\n",
    "    ax = fig.add_subplot(3, 3, i + 1)\n",
    "    ax.imshow(subset[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(202, figsize=(15, 15))\n",
    "subset = np.asarray(band_1_x)[random_indicies]\n",
    "subset = denoise(np.asarray([np.asarray(item).reshape(75, 75) for item in subset]), 0.05, False)\n",
    "for i in range(9):\n",
    "    ax = fig.add_subplot(3, 3, i + 1)\n",
    "    ax.imshow(subset[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(202, figsize=(15, 15))\n",
    "subset = np.asarray(band_1_x)[random_indicies]\n",
    "subset = smooth(denoise(np.asarray(\n",
    "    [np.asarray(item).reshape(75, 75) for item in subset]), 0.05, False), 0.5)\n",
    "for i in range(9):\n",
    "    ax = fig.add_subplot(3, 3, i + 1)\n",
    "    ax.imshow(subset[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_notebook(lr, decay, channels, relu_type='relu'):\n",
    "    # angle variable defines if we should use angle parameter or ignore it\n",
    "    input_1 = Input(shape=(75, 75, channels))\n",
    "\n",
    "    fcnn = Conv2D(32, kernel_size=(3, 3), activation=relu_type)(\n",
    "        BatchNormalization()(input_1))\n",
    "    fcnn = MaxPooling2D((3, 3))(fcnn)\n",
    "    fcnn = Dropout(0.2)(fcnn)\n",
    "    fcnn = Conv2D(64, kernel_size=(3, 3), activation=relu_type)(fcnn)\n",
    "    fcnn = MaxPooling2D((2, 2), strides=(2, 2))(fcnn)\n",
    "    fcnn = Dropout(0.2)(fcnn)\n",
    "    fcnn = Conv2D(128, kernel_size=(3, 3), activation=relu_type)(fcnn)\n",
    "    fcnn = MaxPooling2D((2, 2), strides=(2, 2))(fcnn)\n",
    "    fcnn = Dropout(0.2)(fcnn)\n",
    "    fcnn = Conv2D(128, kernel_size=(3, 3), activation=relu_type)(fcnn)\n",
    "    fcnn = MaxPooling2D((2, 2), strides=(2, 2))(fcnn)\n",
    "    fcnn = Dropout(0.2)(fcnn)\n",
    "    fcnn = BatchNormalization()(fcnn)\n",
    "    fcnn = Flatten()(fcnn)\n",
    "    local_input = input_1\n",
    "    partial_model = Model(input_1, fcnn)\n",
    "    dense = Dropout(0.2)(fcnn)\n",
    "    dense = Dense(256, activation=relu_type)(dense)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    dense = Dense(128, activation=relu_type)(dense)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    dense = Dense(64, activation=relu_type)(dense)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    # For some reason i've decided not to normalize angle data\n",
    "    output = Dense(1, activation=\"sigmoid\")(dense)\n",
    "    model = Model(local_input, output)\n",
    "    optimizer = Adam(lr=lr, decay=decay)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    return model, partial_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, batch_size, epochs, checkpoint_name, X_train, y_train, val_data, verbose=2):\n",
    "    callbacks = [ModelCheckpoint(checkpoint_name, save_best_only=True, monitor='val_loss')]\n",
    "    datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "                                   vertical_flip=True,\n",
    "                                   width_shift_range=0.,\n",
    "                                   height_shift_range=0.,\n",
    "                                   channel_shift_range=0,\n",
    "                                   zoom_range=0.2,\n",
    "                                   rotation_range=10)\n",
    "    x_test, y_test = val_data\n",
    "    try:\n",
    "        model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size), epochs=epochs,\n",
    "                                    steps_per_epoch=len(X_train) / batch_size,\n",
    "                                    validation_data=(x_test, y_test), verbose=1,\n",
    "                                    callbacks=callbacks)\n",
    "    except KeyboardInterrupt:\n",
    "        if verbose > 0:\n",
    "            print('Interrupted')\n",
    "    if verbose > 0:\n",
    "        print('Loading model')\n",
    "    model.load_weights(filepath=checkpoint_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train a particular model\n",
    "def gen_model_weights(lr, decay, channels, relu, batch_size, epochs, path_name, data, verbose=2):\n",
    "    X_train, y_train, X_test, y_test, X_val, y_val = data\n",
    "    model, partial_model = get_model_notebook(lr, decay, channels, relu)\n",
    "    model = train_model(model, batch_size, epochs, path_name,\n",
    "                           X_train, y_train, (X_test, y_test), verbose=verbose)\n",
    "\n",
    "    if verbose > 0:\n",
    "        loss_val, acc_val = model.evaluate(X_val, y_val,\n",
    "                               verbose=0, batch_size=batch_size)\n",
    "\n",
    "        loss_train, acc_train = model.evaluate(X_test, y_test,\n",
    "                                       verbose=0, batch_size=batch_size)\n",
    "\n",
    "        print('Val/Train Loss:', str(loss_val) + '/' + str(loss_train), \\\n",
    "            'Val/Train Acc:', str(acc_val) + '/' + str(acc_train))\n",
    "    return model, partial_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all 3 models\n",
    "def train_models(dataset, lr, batch_size, max_epoch, verbose=2, return_model=False):\n",
    "    y_train, X_b, X_images = dataset\n",
    "    y_train_full, y_val,\\\n",
    "    X_b_full, X_b_val,\\\n",
    "    X_images_full, X_images_val = train_test_split(y_train, X_b, X_images, random_state=687, train_size=0.9)\n",
    "\n",
    "    y_train, y_test, \\\n",
    "    X_b_train, X_b_test, \\\n",
    "    X_images_train, X_images_test = train_test_split(y_train_full, X_b_full, X_images_full, random_state=576, train_size=0.85)\n",
    "\n",
    "    if train_b:\n",
    "        if verbose > 0:\n",
    "            print('Training bandwidth network')\n",
    "        data_b1 = (X_b_train, y_train, X_b_test, y_test, X_b_val, y_val)\n",
    "        model_b, model_b_cut = gen_model_weights(lr, 1e-6, 3, 'relu', batch_size, max_epoch, 'model_b',\n",
    "                                                 data=data_b1, verbose=verbose)\n",
    "\n",
    "    if train_img:\n",
    "        if verbose > 0:\n",
    "            print('Training image network')\n",
    "        data_images = (X_images_train, y_train, X_images_test, y_test, X_images_val, y_val)\n",
    "        model_images, model_images_cut = gen_model_weights(lr, 1e-6, 3, 'relu', batch_size, max_epoch, 'model_img',\n",
    "                                                       data_images, verbose=verbose)\n",
    "\n",
    "    if train_total:\n",
    "        common_model = combined_model(model_b_cut, model_images_cut, lr/2, 1e-7)\n",
    "        common_x_train = [X_b_full, X_images_full]\n",
    "        common_y_train = y_train_full\n",
    "        common_x_val = [X_b_val, X_images_val]\n",
    "        common_y_val = y_val\n",
    "        if verbose > 0:\n",
    "            print('Training common network')\n",
    "        callbacks = [ModelCheckpoint('common', save_best_only=True, monitor='val_loss')]\n",
    "        try:\n",
    "            common_model.fit_generator(gen_flow_multi_inputs(X_b_full, X_images_full, y_train_full, batch_size),\n",
    "                                         epochs=30,\n",
    "                                  steps_per_epoch=len(X_b_full) / batch_size,\n",
    "                                  validation_data=(common_x_val, common_y_val), verbose=1,\n",
    "                                  callbacks=callbacks)\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "        common_model.load_weights(filepath='common')\n",
    "        loss_val, acc_val = common_model.evaluate(common_x_val, common_y_val,\n",
    "                                           verbose=0, batch_size=batch_size)\n",
    "        loss_train, acc_train = common_model.evaluate(common_x_train, common_y_train,\n",
    "                                                  verbose=0, batch_size=batch_size)\n",
    "        if verbose > 0:\n",
    "            print('Loss:', loss_val, 'Acc:', acc_val)\n",
    "    if return_model:\n",
    "        return common_model\n",
    "    else:\n",
    "        return (loss_train, acc_train), (loss_val, acc_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters i got are\n",
    "# epochs : 250\n",
    "# learning rate : 8e-5\n",
    "# batch size : 32\n",
    "common_model = train_models((y_train, X_b, X_images), 7e-04, 32, 50, 1, return_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if predict_submission:\n",
    "    print('Reading test dataset')\n",
    "    test = pd.read_json(\"../input/test.json\")\n",
    "    y_fin, X_fin_b, X_fin_img = create_dataset(test, False)\n",
    "    print('Predicting')\n",
    "    prediction = common_model.predict([X_fin_b, X_fin_img], verbose=1, batch_size=32)\n",
    "    print('Submitting')\n",
    "    submission = pd.DataFrame({'id': test[\"id\"], 'is_iceberg': prediction.reshape((prediction.shape[0]))})\n",
    "\n",
    "    submission.to_csv(\"./submission.csv\", index=False)\n",
    "    print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
